{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#План-действий:\" data-toc-modified-id=\"План-действий:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>План действий:</a></span></li><li><span><a href=\"#Подготовка-данных\" data-toc-modified-id=\"Подготовка-данных-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Подготовка данных</a></span></li><li><span><a href=\"#Исследование-задачи\" data-toc-modified-id=\"Исследование-задачи-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Исследование задачи</a></span></li><li><span><a href=\"#Борьба-с-дисбалансом\" data-toc-modified-id=\"Борьба-с-дисбалансом-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Борьба с дисбалансом</a></span></li><li><span><a href=\"#Тестирование-модели\" data-toc-modified-id=\"Тестирование-модели-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Тестирование модели</a></span></li><li><span><a href=\"#Чек-лист-готовности-проекта\" data-toc-modified-id=\"Чек-лист-готовности-проекта-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Чек-лист готовности проекта</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План действий:\n",
    "1. Ознакомиться с файлом, изучить его, подготовим данные\n",
    "2. Исследуем баланс классов, обучим модель без учета дисбаланса\n",
    "3. Исследовать качество моделей, изменив дисбаланс\n",
    "4. Проверить качество модели на тестовой выборке\n",
    "5. Сделать общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "RowNumber          10000 non-null int64\n",
      "CustomerId         10000 non-null int64\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             9091 non-null float64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "5          6    15574012       Chu          645     Spain    Male   44   \n",
      "6          7    15592531  Bartlett          822    France    Male   50   \n",
      "7          8    15656148    Obinna          376   Germany  Female   29   \n",
      "8          9    15792365        He          501    France    Male   44   \n",
      "9         10    15592389        H?          684    France    Male   27   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0     2.0       0.00              1          1               1   \n",
      "1     1.0   83807.86              1          0               1   \n",
      "2     8.0  159660.80              3          1               0   \n",
      "3     1.0       0.00              2          0               0   \n",
      "4     2.0  125510.82              1          1               1   \n",
      "5     8.0  113755.78              2          1               0   \n",
      "6     7.0       0.00              2          1               1   \n",
      "7     4.0  115046.74              4          1               0   \n",
      "8     4.0  142051.07              2          0               1   \n",
      "9     2.0  134603.88              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n",
      "5        149756.71       1  \n",
      "6         10062.80       0  \n",
      "7        119346.88       1  \n",
      "8         74940.50       0  \n",
      "9         71725.73       0  \n",
      "         RowNumber    CustomerId   CreditScore           Age       Tenure  \\\n",
      "count  10000.00000  1.000000e+04  10000.000000  10000.000000  9091.000000   \n",
      "mean    5000.50000  1.569094e+07    650.528800     38.921800     4.997690   \n",
      "std     2886.89568  7.193619e+04     96.653299     10.487806     2.894723   \n",
      "min        1.00000  1.556570e+07    350.000000     18.000000     0.000000   \n",
      "25%     2500.75000  1.562853e+07    584.000000     32.000000     2.000000   \n",
      "50%     5000.50000  1.569074e+07    652.000000     37.000000     5.000000   \n",
      "75%     7500.25000  1.575323e+07    718.000000     44.000000     7.000000   \n",
      "max    10000.00000  1.581569e+07    850.000000     92.000000    10.000000   \n",
      "\n",
      "             Balance  NumOfProducts    HasCrCard  IsActiveMember  \\\n",
      "count   10000.000000   10000.000000  10000.00000    10000.000000   \n",
      "mean    76485.889288       1.530200      0.70550        0.515100   \n",
      "std     62397.405202       0.581654      0.45584        0.499797   \n",
      "min         0.000000       1.000000      0.00000        0.000000   \n",
      "25%         0.000000       1.000000      0.00000        0.000000   \n",
      "50%     97198.540000       1.000000      1.00000        1.000000   \n",
      "75%    127644.240000       2.000000      1.00000        1.000000   \n",
      "max    250898.090000       4.000000      1.00000        1.000000   \n",
      "\n",
      "       EstimatedSalary        Exited  \n",
      "count     10000.000000  10000.000000  \n",
      "mean     100090.239881      0.203700  \n",
      "std       57510.492818      0.402769  \n",
      "min          11.580000      0.000000  \n",
      "25%       51002.110000      0.000000  \n",
      "50%      100193.915000      0.000000  \n",
      "75%      149388.247500      0.000000  \n",
      "max      199992.480000      1.000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.info()\n",
    "print(df.head(10))\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом видно, что мы имеем датафрейм, состоящий из 10000 строк и 14 признаков, из них 3 являются float64, 8 - int64 и 3- object. Также видно, что столбце Tenure имеет 909 пропущенных значений, которые необходимо заполнить. Также можно заметить, что 25% имеют 0 баланс, скорее всего клиенты банка просто вывели свои деньги из банка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем название столбцов к нормльному виду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>surname</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10.0</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_number  customer_id    surname  credit_score geography  gender  age  \\\n",
       "0              1     15634602   Hargrave           619    France  Female   42   \n",
       "1              2     15647311       Hill           608     Spain  Female   41   \n",
       "2              3     15619304       Onio           502    France  Female   42   \n",
       "3              4     15701354       Boni           699    France  Female   39   \n",
       "4              5     15737888   Mitchell           850     Spain  Female   43   \n",
       "...          ...          ...        ...           ...       ...     ...  ...   \n",
       "9995        9996     15606229   Obijiaku           771    France    Male   39   \n",
       "9996        9997     15569892  Johnstone           516    France    Male   35   \n",
       "9997        9998     15584532        Liu           709    France  Female   36   \n",
       "9998        9999     15682355  Sabbatini           772   Germany    Male   42   \n",
       "9999       10000     15628319     Walker           792    France  Female   28   \n",
       "\n",
       "      tenure    balance  num_of_products  has_cr_card  is_active_member  \\\n",
       "0        2.0       0.00                1            1                 1   \n",
       "1        1.0   83807.86                1            0                 1   \n",
       "2        8.0  159660.80                3            1                 0   \n",
       "3        1.0       0.00                2            0                 0   \n",
       "4        2.0  125510.82                1            1                 1   \n",
       "...      ...        ...              ...          ...               ...   \n",
       "9995     5.0       0.00                2            1                 0   \n",
       "9996    10.0   57369.61                1            1                 1   \n",
       "9997     7.0       0.00                1            0                 1   \n",
       "9998     3.0   75075.31                2            1                 0   \n",
       "9999     NaN  130142.79                1            1                 0   \n",
       "\n",
       "      estimated_salary  exited  \n",
       "0            101348.88       1  \n",
       "1            112542.58       0  \n",
       "2            113931.57       1  \n",
       "3             93826.63       0  \n",
       "4             79084.10       0  \n",
       "...                ...     ...  \n",
       "9995          96270.64       0  \n",
       "9996         101699.77       0  \n",
       "9997          42085.58       1  \n",
       "9998          92888.52       1  \n",
       "9999          38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['row_number', 'customer_id', 'surname', 'credit_score', 'geography', 'gender', 'age', 'tenure', 'balance', 'num_of_products', 'has_cr_card', 'is_active_member', 'estimated_salary', 'exited']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим ненужные столбцы, которые являются информацией для знакомства с клиентом, но не влияют уйдет ли клиент из банка, а именно столбцы: row_number, surname, customer_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['row_number', 'surname', 'customer_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим пропущенные значения в столбце \"Tenure\" на медианное значение по этому столбцу. Возможно это новые пользователи или те, которые уже вышли, но так как некоторые уже покинули банк, то скорее всего это клиенты, которые уже несколько лет в банке, поэтому лучше заменить значение на медианное. Также среднее значение и медианное в данном столбце примерно равны. Также можно удалить данные значения, так как то, сколько лет провел клиент в банке не сильно повлияет на его уход, он может уйти в первый год, а может и через 5 лет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(10000, 11)\n"
     ]
    }
   ],
   "source": [
    "df_first = df.fillna(value = 5)\n",
    "print(df_first['tenure'].isna().sum())\n",
    "print(df_first.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создали новый датафрейм, в котором нет пропущенных значений. Но также сделаем новый датасет, в котором будут удалены пропущенные значения и в дальнейшем проверим как изменится качество модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9091, 11)\n"
     ]
    }
   ],
   "source": [
    "df_new = df.dropna().reset_index(drop = True)\n",
    "print(df_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом было создано два датасета, для того, чтобы рассмотреть две разные модели и увидеть, качество какой модели будет лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем категориальные признаки в численные для обоих датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_ohe = pd.get_dummies(df_first, drop_first = True)\n",
    "df_new_ohe = pd.get_dummies(df_new, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Таким образом была произведена подготовка данных к дальнейшему исследованию, были заменены категориальные данные на целочисленные, создано два датасета для их дальнейшего исследования: с заменой пропущенных значений и с удалением пропущенных значений, также были удалены ненудные столбцы**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследование задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с первым датасетом с заменой значений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборки на тренировочную (которая разделится на обучающую и валидационную) и тестовую выборки. Применим для этого метод sample. По правилам деления выборки должны разделиться на 60 - 20 -20 %, т.е. они должны равняться 6000 - 2000 - 2000. Для начала разделим выборку на тренировочную и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 12)\n",
      "(2000, 12)\n"
     ]
    }
   ],
   "source": [
    "train_first = df_first_ohe.sample(frac = 0.8, random_state = 123).copy()\n",
    "print(train_first.shape)\n",
    "\n",
    "test_first = df_first_ohe[~df_first_ohe.index.isin(train_first.index)].copy()\n",
    "print(test_first.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом данные были разделены на обучающую и тестовую выборки. Но для проверки качества модели нужна также валидационная выборка, получим её из тренировочной выборки, чтобы получить число 2000, нужно взять 25 процентов от размера train_first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 12)\n",
      "(2000, 12)\n"
     ]
    }
   ],
   "source": [
    "df_train_first, df_valid_first = train_test_split(train_first, test_size=0.25, random_state=123)\n",
    "print(df_train_first.shape)\n",
    "print(df_valid_first.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним признаки и цели для каждой выборки в отдельных переменных features_train_first, target_train_first; features_valid_first, target_valid_first; features_test_first, target_test_first и проверим их размер, чтобы убедиться, что в признаках осталось 11 столбцов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 11)\n",
      "(6000,)\n",
      "(2000, 11)\n",
      "(2000,)\n",
      "(2000, 11)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "features_train_first = df_train_first.drop(['exited'], axis=1)\n",
    "target_train_first = df_train_first['exited']\n",
    "\n",
    "features_valid_first = df_valid_first.drop(['exited'], axis=1)\n",
    "target_valid_first = df_valid_first['exited']\n",
    "\n",
    "features_test_first = test_first.drop(['exited'], axis=1)\n",
    "target_test_first = test_first['exited']\n",
    "\n",
    "print(features_train_first.shape)\n",
    "print(target_train_first.shape)\n",
    "\n",
    "print(features_valid_first.shape)\n",
    "print(target_valid_first.shape)\n",
    "\n",
    "print(features_test_first.shape)\n",
    "print(target_test_first.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом были разделены данные на тренировочную, валидационную и тестовую выборки в соотношении 60:20:20. Далее приступим к стандартизации классов методом масштабирования признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>geography_Germany</th>\n",
       "      <th>geography_Spain</th>\n",
       "      <th>gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2256</td>\n",
       "      <td>0.479630</td>\n",
       "      <td>-0.568592</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.393707</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.609849</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9202</td>\n",
       "      <td>-1.257624</td>\n",
       "      <td>-0.759496</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.715344</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.638045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8544</td>\n",
       "      <td>0.407245</td>\n",
       "      <td>0.672285</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.393582</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.459033</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8818</td>\n",
       "      <td>0.283155</td>\n",
       "      <td>-0.186784</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.230201</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.065188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8017</td>\n",
       "      <td>-1.971139</td>\n",
       "      <td>1.054093</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.529178</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984366</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      credit_score       age  tenure   balance  num_of_products  has_cr_card  \\\n",
       "2256      0.479630 -0.568592     2.0  1.393707                1            1   \n",
       "9202     -1.257624 -0.759496     9.0  0.715344                1            1   \n",
       "8544      0.407245  0.672285     7.0 -0.393582                2            1   \n",
       "8818      0.283155 -0.186784     3.0 -1.230201                2            1   \n",
       "8017     -1.971139  1.054093     5.0  0.529178                1            1   \n",
       "\n",
       "      is_active_member  estimated_salary  geography_Germany  geography_Spain  \\\n",
       "2256                 1         -1.609849                  0                0   \n",
       "9202                 0         -1.638045                  0                0   \n",
       "8544                 1         -0.459033                  0                0   \n",
       "8818                 1         -1.065188                  0                0   \n",
       "8017                 0          0.984366                  1                0   \n",
       "\n",
       "      gender_Male  \n",
       "2256            1  \n",
       "9202            1  \n",
       "8544            0  \n",
       "8818            0  \n",
       "8017            1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выделяем те признаки, которые необходимо масштабировать\n",
    "numeric = ['credit_score', 'age', 'balance', 'estimated_salary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train_first[numeric])\n",
    "features_train_first[numeric] =  scaler.transform(features_train_first[numeric])\n",
    "features_valid_first[numeric] =  scaler.transform(features_valid_first[numeric])\n",
    "features_test_first[numeric] =  scaler.transform(features_test_first[numeric])\n",
    "features_train_first.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим сбалансированы ли данные по целевому признаку методом value_counts()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7963\n",
       "1    2037\n",
       "Name: exited, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_first_ohe['exited'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить данные не сбалансированы, они примерно соотносятся как 4:1. Попробуем построить модель по данным с дисбалансом и проверим качество модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Начнем проверку с Дерева решений**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8385\n",
      "F1: 0.4262877442273535\n",
      "ROC_AUC: 0.7817634680334605\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=123, max_depth = 3)\n",
    "model.fit(features_train_first, target_train_first)\n",
    "predicted_valid_first = model.predict(features_valid_first)\n",
    "\n",
    "print('Accuracy:', accuracy_score(target_valid_first, predicted_valid_first))\n",
    "print('F1:', f1_score(target_valid_first, predicted_valid_first))\n",
    "\n",
    "#Сравним модель со случайной\n",
    "probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "\n",
    "print('ROC_AUC:', roc_auc_score(target_valid_first, probabilities_one_valid_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Случайный лес**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.851\n",
      "F1: 0.5130718954248367\n",
      "ROC_AUC: 0.8072507208443973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=123)\n",
    "model.fit(features_train_first, target_train_first)\n",
    "predicted_valid_first = model.predict(features_valid_first)\n",
    "\n",
    "print('Accuracy:', accuracy_score(target_valid_first, predicted_valid_first))\n",
    "print('F1:', f1_score(target_valid_first, predicted_valid_first))\n",
    "\n",
    "#Сравним модель со случайной\n",
    "probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "\n",
    "print('ROC_AUC:', roc_auc_score(target_valid_first, probabilities_one_valid_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8185\n",
      "F1: 0.3059273422562141\n",
      "ROC_AUC: 0.7752263871009626\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=123, solver='liblinear')\n",
    "model.fit(features_train_first, target_train_first)\n",
    "predicted_valid_first = model.predict(features_valid_first)\n",
    "\n",
    "print('Accuracy:', accuracy_score(target_valid_first, predicted_valid_first))\n",
    "print('F1:', f1_score(target_valid_first, predicted_valid_first))\n",
    "\n",
    "#Сравним модель со случайной\n",
    "probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "\n",
    "print('ROC_AUC:', roc_auc_score(target_valid_first, probabilities_one_valid_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить для всех трех моделей точность высока, также как и значение AUC_ROC - площадь под кривой, но данные модели плохо предсказывают целевой признак со значением класс 1, только случайный лес показывает значение превышающее 0.51, однако этого не хватает до 0.59, поэтому необходимо бороться с дисбалансом классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name_of_method</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>DesicionTree</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.426288</td>\n",
       "      <td>0.781763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.8510</td>\n",
       "      <td>0.513072</td>\n",
       "      <td>0.807251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.8185</td>\n",
       "      <td>0.305927</td>\n",
       "      <td>0.775226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      name_of_method  Accuracy        F1   ROC_AUC\n",
       "0   1        DesicionTree    0.8385  0.426288  0.781763\n",
       "1   2        RandomForest    0.8510  0.513072  0.807251\n",
       "2   3  LogisticRegression    0.8185  0.305927  0.775226"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.DataFrame([[1,'DesicionTree', 0.8385, 0.4262877442273535, 0.7817634680334605],\n",
    "                  [2,'RandomForest', 0.851, 0.5130718954248367, 0.8072507208443973],\n",
    "                  [3,'LogisticRegression', 0.8185, 0.3059273422562141, 0.7752263871009626]], \n",
    "columns=['id','name_of_method', 'Accuracy', 'F1', 'ROC_AUC'])\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа со вторым датасетом с удаленными значениями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборки на тренировочную (которая разделится на обучающую и валидационную) и тестовую выборки. Применим для этого метод sample. По правилам деления выборки должны разделиться на 60 - 20 -20 %, т.е. они должны равняться 6000 - 2000 - 2000. Для начала разделим выборку на тренировочную и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7273, 12)\n",
      "(1818, 12)\n"
     ]
    }
   ],
   "source": [
    "train_new = df_new_ohe.sample(frac = 0.8, random_state = 123).copy()\n",
    "print(train_new.shape)\n",
    "\n",
    "test_new = df_new_ohe[~df_new_ohe.index.isin(train_new.index)].copy()\n",
    "print(test_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом данные были разделены на обучающую и тестовую выборки. Но для проверки качества модели нужна также валидационная выборка, получим её из тренировочной выборки, чтобы получить число 2000, нужно взять 25 процентов от размера train_new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5454, 12)\n",
      "(1819, 12)\n"
     ]
    }
   ],
   "source": [
    "df_train_new, df_valid_new = train_test_split(train_new, test_size=0.25, random_state=123)\n",
    "print(df_train_new.shape)\n",
    "print(df_valid_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним признаки и цели для каждой выборки в отдельных переменных features_train_new, target_train_new; features_valid_new, target_valid_new; features_test_new, target_test_new и проверим их размер, чтобы убедиться, что в признаках осталось 11 столбцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5454, 11)\n",
      "(5454,)\n",
      "(1819, 11)\n",
      "(1819,)\n",
      "(1818, 11)\n",
      "(1818,)\n"
     ]
    }
   ],
   "source": [
    "features_train_new = df_train_new.drop(['exited'], axis=1)\n",
    "target_train_new = df_train_new['exited']\n",
    "\n",
    "features_valid_new = df_valid_new.drop(['exited'], axis=1)\n",
    "target_valid_new = df_valid_new['exited']\n",
    "\n",
    "features_test_new = test_new.drop(['exited'], axis=1)\n",
    "target_test_new = test_new['exited']\n",
    "\n",
    "print(features_train_new.shape)\n",
    "print(target_train_new.shape)\n",
    "\n",
    "print(features_valid_new.shape)\n",
    "print(target_valid_new.shape)\n",
    "\n",
    "print(features_test_new.shape)\n",
    "print(target_test_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом были разделены данные на тренировочную, валидационную и тестовую выборки в соотношении 60:20:20. Далее приступим к стандартизации классов методом масштабирования признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>geography_Germany</th>\n",
       "      <th>geography_Spain</th>\n",
       "      <th>gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8414</td>\n",
       "      <td>-0.562713</td>\n",
       "      <td>-0.942337</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.226491</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.717891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5254</td>\n",
       "      <td>-0.500801</td>\n",
       "      <td>0.276889</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.226491</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.205398</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7154</td>\n",
       "      <td>0.066725</td>\n",
       "      <td>0.464462</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.003919</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.372328</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2834</td>\n",
       "      <td>0.448515</td>\n",
       "      <td>-0.192044</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.072342</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.166758</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5592</td>\n",
       "      <td>-1.047689</td>\n",
       "      <td>-0.942337</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.226491</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.791544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      credit_score       age  tenure   balance  num_of_products  has_cr_card  \\\n",
       "8414     -0.562713 -0.942337     2.0 -1.226491                2            1   \n",
       "5254     -0.500801  0.276889    10.0 -1.226491                2            0   \n",
       "7154      0.066725  0.464462     6.0 -0.003919                1            1   \n",
       "2834      0.448515 -0.192044    10.0  1.072342                1            0   \n",
       "5592     -1.047689 -0.942337     3.0 -1.226491                2            1   \n",
       "\n",
       "      is_active_member  estimated_salary  geography_Germany  geography_Spain  \\\n",
       "8414                 1         -1.717891                  0                1   \n",
       "5254                 0          1.205398                  0                0   \n",
       "7154                 0         -0.372328                  0                0   \n",
       "2834                 1         -1.166758                  0                0   \n",
       "5592                 0          0.791544                  0                0   \n",
       "\n",
       "      gender_Male  \n",
       "8414            1  \n",
       "5254            0  \n",
       "7154            1  \n",
       "2834            1  \n",
       "5592            1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выделяем те признаки, которые необходимо масштабировать\n",
    "numeric = ['credit_score', 'age', 'balance', 'estimated_salary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train_new[numeric])\n",
    "features_train_new[numeric] =  scaler.transform(features_train_new[numeric])\n",
    "features_valid_new[numeric] =  scaler.transform(features_valid_new[numeric])\n",
    "features_test_new[numeric] =  scaler.transform(features_test_new[numeric])\n",
    "features_train_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было замечено выше - данные не сбалансированы, они примерно соотносятся как 4:1. Попробуем построить модель по данным с дисбалансом и проверим качество модели. **Начнем с Обучающего дерева**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8416712479384277\n",
      "F1: 0.4330708661417323\n",
      "ROC_AUC: 0.784687471908429\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=123, max_depth = 3)\n",
    "model.fit(features_train_new, target_train_new)\n",
    "predicted_valid_new = model.predict(features_valid_new)\n",
    "\n",
    "print('Accuracy:', accuracy_score(target_valid_new, predicted_valid_new))\n",
    "print('F1:', f1_score(target_valid_new, predicted_valid_new))\n",
    "\n",
    "#Сравним модель со случайной\n",
    "probabilities_valid_new = model.predict_proba(features_valid_new)\n",
    "probabilities_one_valid_new = probabilities_valid_new[:, 1]\n",
    "\n",
    "print('ROC_AUC:', roc_auc_score(target_valid_new, probabilities_one_valid_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Случайный лес**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8532160527762507\n",
      "F1: 0.5420240137221269\n",
      "ROC_AUC: 0.8184011026878015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=123)\n",
    "model.fit(features_train_new, target_train_new)\n",
    "predicted_valid_new = model.predict(features_valid_new)\n",
    "\n",
    "print('Accuracy:', accuracy_score(target_valid_new, predicted_valid_new))\n",
    "print('F1:', f1_score(target_valid_new, predicted_valid_new))\n",
    "\n",
    "#Сравним модель со случайной\n",
    "probabilities_valid_new = model.predict_proba(features_valid_new)\n",
    "probabilities_one_valid_new = probabilities_valid_new[:, 1]\n",
    "\n",
    "print('ROC_AUC:', roc_auc_score(target_valid_new, probabilities_one_valid_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8158328752061572\n",
      "F1: 0.31492842535787324\n",
      "ROC_AUC: 0.7572494980972644\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=123, solver='liblinear')\n",
    "model.fit(features_train_new, target_train_new)\n",
    "predicted_valid_new = model.predict(features_valid_new)\n",
    "\n",
    "print('Accuracy:', accuracy_score(target_valid_new, predicted_valid_new))\n",
    "print('F1:', f1_score(target_valid_new, predicted_valid_new))\n",
    "\n",
    "#Сравним модель со случайной\n",
    "probabilities_valid_new = model.predict_proba(features_valid_new)\n",
    "probabilities_one_valid_new = probabilities_valid_new[:, 1]\n",
    "\n",
    "print('ROC_AUC:', roc_auc_score(target_valid_new, probabilities_one_valid_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как можно заметить для всех трех моделей точность высока, также как и значение AUC_ROC - площадь под кривой, но данные модели плохо предсказывают целевой признак со значением класс 1, только случайный лес показывает значение превышающее 0.54, однако этого не хватает до 0.59, поэтому необходимо бороться с дисбалансом классов.**\n",
    "\n",
    "**Однако можно заметить, что для второго варианта, когда удаляются пропущенные значения качество моделей немного выше, чем в случае с заменой пропущенных значений.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Борьба с дисбалансом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С дисбалансом признаков борятся многими методами, в данном проекте рассмотрим 3 из них: метод взвешивания классов, метод downsampling, метод upsampling. Первым рассмотрим метод взвешивание классов для каждой модели: обучающее дерево, случайный лес и логистическая регрессия. **Применим данный метод к первому датасету, в которойй производилась замена пропущенных значений.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Взвешивание классов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучающее дерево**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 1 : 0.4749754661432777\n",
      "ROC_AUC: 0.6884107538425243\n",
      "max_depth = 2 : 0.4894837476099427\n",
      "ROC_AUC: 0.7256959453789164\n",
      "max_depth = 3 : 0.4894837476099427\n",
      "ROC_AUC: 0.7797248361573333\n",
      "max_depth = 4 : 0.5426944971537002\n",
      "ROC_AUC: 0.8152421577988158\n",
      "max_depth = 5 : 0.5303292894280762\n",
      "ROC_AUC: 0.8208489892344402\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "for depth in range(1, 6):\n",
    "    model = DecisionTreeClassifier(random_state=123, max_depth=depth, class_weight = \"balanced\")\n",
    "    model.fit(features_train_first, target_train_first)\n",
    "    predictions_valid_first = model.predict(features_valid_first) \n",
    "    print(\"max_depth =\", depth, \": \", end='')\n",
    "    print(f1_score(target_valid_first, predictions_valid_first))\n",
    "    probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "    probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "    print('ROC_AUC:', roc_auc_score(target_valid_first, probabilities_one_valid_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Максимальное качество 0.54 достигается при количестве деревьев = 4, но этого не достаточно по условиям, однако при лучшем балансе классов, значение метрики увеличилось с 0,51 до 0.54, также как и показатель ROC_AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Случайный лес**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели на валидационной выборке: 0.5643153526970954 Количество деревьев: 7 Максимальная глубина: 4 Максимальный AUC_ROC: 0.8259982170877728\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(1, 11):\n",
    "    for depth in range(1, 6):\n",
    "        model = RandomForestClassifier(random_state=123, n_estimators=est, max_depth = depth, class_weight = \"balanced\")\n",
    "        model.fit(features_train_first, target_train_first)\n",
    "        predicted_valid_first = model.predict(features_valid_first)\n",
    "        result = f1_score(target_valid_first, predicted_valid_first)\n",
    "        probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "        probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "        auc_roc = roc_auc_score(target_valid_first, probabilities_one_valid_first)\n",
    "        if result > best_result:\n",
    "            best_model = model \n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            best_auc_roc = auc_roc\n",
    "            \n",
    "print(\"F1-мера наилучшей модели на валидационной выборке:\", best_result, \"Количество деревьев:\", best_est, \"Максимальная глубина:\", best_depth, \"Максимальный AUC_ROC:\", best_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случайного леса показатели метрики увеличилсь и стали 0.56, также они увеличились по сравнению с несбалансированным датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5036496350364964\n",
      "ROC_AUC: 0.7792909836260256\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=123, solver='liblinear', class_weight = \"balanced\")\n",
    "model.fit(features_train_first, target_train_first)\n",
    "predicted_valid_first = model.predict(features_valid_first)\n",
    "print('F1:', f1_score(target_valid_first, predicted_valid_first))\n",
    "probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "print('ROC_AUC:', roc_auc_score(target_valid_first, probabilities_one_valid_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить балансирование классов значительно влияет на показатели метрики для логистической регрессии, он вырос с 0.3 до 0.5, однако этого еще не достаточно по условию задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Таким образом, можно заметить, что взвешивание классов увеличивает показатели качества по сравнению с несбалансированными данными, однако этих показателей еще не достаточно для выполнения условий задачи, поэтому перейдем к следующему способу борьбы с дисбалансом: upsampling.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPsampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Разделим обучающую выборку на отрицательные и положительные объекты;\n",
    "- Скопируем несколько раз положительные объекты;\n",
    "- С учётом полученных данных создадим новую обучающую выборку;\n",
    "- Перемешаем данные: идущие друг за другом одинаковые вопросы не помогут обучению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=123)\n",
    "    \n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "# В качестве повтора поставим значение = 4, так как количество ответов класса 1 в 4 раза меньше, чем ответов класса 0\n",
    "features_upsampled_first, target_upsampled_first = upsample(features_train_first, target_train_first, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    4884\n",
      "0    4779\n",
      "Name: exited, dtype: int64\n",
      "0    4779\n",
      "1    1221\n",
      "Name: exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(target_upsampled_first.value_counts())\n",
    "print(target_train_first.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить увеличение выборки прошло успешно, данные сбалансированы, можно приступить к проверке качества моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучающее дерево**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 1 : 0.4749754661432777\n",
      "ROC_AUC: 0.6884107538425243\n",
      "max_depth = 2 : 0.4894837476099427\n",
      "ROC_AUC: 0.7256959453789164\n",
      "max_depth = 3 : 0.4894837476099427\n",
      "ROC_AUC: 0.7797248361573333\n",
      "max_depth = 4 : 0.5426944971537002\n",
      "ROC_AUC: 0.8152421577988158\n",
      "max_depth = 5 : 0.5303292894280762\n",
      "ROC_AUC: 0.8208489892344402\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "for depth in range(1, 6):\n",
    "    model = DecisionTreeClassifier(random_state=123, max_depth=depth)\n",
    "    model.fit(features_upsampled_first, target_upsampled_first)\n",
    "    predictions_valid_first = model.predict(features_valid_first) \n",
    "    print(\"max_depth =\", depth, \": \", end='')\n",
    "    print(f1_score(target_valid_first, predictions_valid_first))\n",
    "    probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "    probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "    print('ROC_AUC:', roc_auc_score(target_valid_first, probabilities_one_valid_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Не совсем понимаю, почему для данной модели показатели метрик не изменились по сравнению со взвешиванием, хотя для случайного леса (ниже), поменялось.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Случайный лес**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели на валидационной выборке: 0.5828092243186582 Количество деревьев: 28 Максимальная глубина: 5 Максимальный AUC_ROC: 0.8435280761173681\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(1, 100):\n",
    "    for depth in range(1, 6):\n",
    "        model = RandomForestClassifier(random_state=123, n_estimators=est, max_depth = depth)\n",
    "        model.fit(features_upsampled_first, target_upsampled_first)\n",
    "        predicted_valid_first = model.predict(features_valid_first)\n",
    "        result = f1_score(target_valid_first, predicted_valid_first)\n",
    "        probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "        probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "        auc_roc = roc_auc_score(target_valid_first, probabilities_one_valid_first)\n",
    "        if result > best_result:\n",
    "            best_model = model \n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            best_auc_roc = auc_roc\n",
    "            \n",
    "print(\"F1-мера наилучшей модели на валидационной выборке:\", best_result, \"Количество деревьев:\", best_est, \"Максимальная глубина:\", best_depth, \"Максимальный AUC_ROC:\", best_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случайного леса показатели метрики увеличилсь и стали 0.582, по сравнению со взвешиванием, также они увеличились по сравнению с несбалансированным датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5\n",
      "ROC_AUC: 0.7793353189211956\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=123, solver='liblinear')\n",
    "model.fit(features_upsampled_first, target_upsampled_first)\n",
    "predicted_valid_first = model.predict(features_valid_first)\n",
    "print('F1:', f1_score(target_valid_first, predicted_valid_first))\n",
    "probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "print('ROC_AUC:', roc_auc_score(target_valid_first, probabilities_one_valid_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить upsampling не изменил показатели качества для логистической регрессии.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, можно заметить, что upsampling увеличивает показатели качества по сравнению с несбалансированными данными и по сравнению со взвешиванием классов для модели случайный лес, а для моделей обучающее дерево и логистическая регрессия показатели качества меняются незначительно, однако показателей случайного леса еще не достаточно для выполнения условий задачи, поэтому перейдем к следующему способу борьбы с дисбалансом: downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOWNsampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Разделим обучающую выборку на отрицательные и положительные объекты;\n",
    "- Случайным образом отбросим часть из отрицательных объектов;\n",
    "- С учётом полученных данных создадим новую обучающую выборку;\n",
    "- Перемешаем данные. Положительные не должны идти следом за отрицательными: алгоритмам будет сложнее обучаться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=123)] + [features_ones])\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=123)] + [target_ones])\n",
    "    \n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=123)\n",
    "    \n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "#Так как данные отличаются в соотношении 1:4, то уменьшим больший класс и поставим значение fraction = 0.25\n",
    "features_downsampled_first, target_downsampled_first = downsample(features_train_first, target_train_first, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1221\n",
      "0    1195\n",
      "Name: exited, dtype: int64\n",
      "0    4779\n",
      "1    1221\n",
      "Name: exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(target_downsampled_first.value_counts())\n",
    "print(target_train_first.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить уменьшение выборки прошло успешно, данные сбалансированы, можно приступить к проверке качества моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучающее дерево**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 1 : 0.4749754661432777\n",
      "ROC_AUC: 0.6884107538425243\n",
      "max_depth = 2 : 0.4894837476099427\n",
      "ROC_AUC: 0.7200020267563506\n",
      "max_depth = 3 : 0.5417721518987342\n",
      "ROC_AUC: 0.7754710229260979\n",
      "max_depth = 4 : 0.48874061718098416\n",
      "ROC_AUC: 0.8037355652987644\n",
      "max_depth = 5 : 0.535059331175836\n",
      "ROC_AUC: 0.8055960642925116\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "for depth in range(1, 6):\n",
    "    model = DecisionTreeClassifier(random_state=123, max_depth=depth)\n",
    "    model.fit(features_downsampled_first, target_downsampled_first)\n",
    "    predictions_valid_first = model.predict(features_valid_first) \n",
    "    print(\"max_depth =\", depth, \": \", end='')\n",
    "    print(f1_score(target_valid_first, predictions_valid_first))\n",
    "    probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "    probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "    print('ROC_AUC:', roc_auc_score(target_valid_first, probabilities_one_valid_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить метрики качества для обучающего дерева увеличились до 0.542"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Случайный лес**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели на валидационной выборке: 0.5829244357212953 Количество деревьев: 5 Максимальная глубина: 5 Максимальный AUC_ROC: 0.8366236455963176\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(1, 100):\n",
    "    for depth in range(1, 6):\n",
    "        model = RandomForestClassifier(random_state=123, n_estimators=est, max_depth = depth)\n",
    "        model.fit(features_downsampled_first, target_downsampled_first)\n",
    "        predicted_valid_first = model.predict(features_valid_first)\n",
    "        result = f1_score(target_valid_first, predicted_valid_first)\n",
    "        probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "        probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "        auc_roc = roc_auc_score(target_valid_first, probabilities_one_valid_first)\n",
    "        if result > best_result:\n",
    "            best_model = model \n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            best_auc_roc = auc_roc\n",
    "            \n",
    "print(\"F1-мера наилучшей модели на валидационной выборке:\", best_result, \"Количество деревьев:\", best_est, \"Максимальная глубина:\", best_depth, \"Максимальный AUC_ROC:\", best_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случайного леса показатели метрики увеличилсь и стали 0.58, по сравнению со взвешиванием и upsampling, также они увеличились по сравнению с несбалансированным датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5114155251141552\n",
      "ROC_AUC: 0.7790946416045577\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=123, solver='liblinear')\n",
    "model.fit(features_downsampled_first, target_downsampled_first)\n",
    "predicted_valid_first = model.predict(features_valid_first)\n",
    "print('F1:', f1_score(target_valid_first, predicted_valid_first))\n",
    "probabilities_valid_first = model.predict_proba(features_valid_first)\n",
    "probabilities_one_valid_first = probabilities_valid_first[:, 1]\n",
    "print('ROC_AUC:', roc_auc_score(target_valid_first, probabilities_one_valid_first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить downsampling изменил показатели качества для логистической регрессии c 0.5 до 0.51."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, можно заметить, что downsampling увеличивает показатели качества по сравнению с несбалансированными данными и по сравнению со взвешиванием классов и upsampling для всех моделей, однако показателей еще не достаточно для выполнения условий задачи, поэтому перейдем к другому датасету"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Датасет с удаленными пропущенными значениями**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод downsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_downsampled_new, target_downsampled_new = downsample(features_train_new, target_train_new, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1101\n",
      "0    1088\n",
      "Name: exited, dtype: int64\n",
      "0    4353\n",
      "1    1101\n",
      "Name: exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(target_downsampled_new.value_counts())\n",
    "print(target_train_new.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить уменьшение произведено корректно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели на валидационной выборке: 0.5859030837004404 Количество деревьев: 14 Максимальная глубина: 5 Максимальный AUC_ROC: 0.8385024945315075\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(1, 100):\n",
    "    for depth in range(1, 6):\n",
    "        model = RandomForestClassifier(random_state=123, n_estimators=est, max_depth = depth)\n",
    "        model.fit(features_downsampled_new, target_downsampled_new)\n",
    "        predicted_valid_new = model.predict(features_valid_new)\n",
    "        result = f1_score(target_valid_new, predicted_valid_new)\n",
    "        probabilities_valid_new = model.predict_proba(features_valid_new)\n",
    "        probabilities_one_valid_new = probabilities_valid_new[:, 1]\n",
    "        auc_roc = roc_auc_score(target_valid_new, probabilities_one_valid_new)\n",
    "        if result > best_result:\n",
    "            best_model = model \n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            best_auc_roc = auc_roc\n",
    "            \n",
    "print(\"F1-мера наилучшей модели на валидационной выборке:\", best_result, \"Количество деревьев:\", best_est, \"Максимальная глубина:\", best_depth, \"Максимальный AUC_ROC:\", best_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили качество близкое к нужному, но еще недостаточное, проверим метод upsampling для случайного леса, так как именно для этой модели получаются лучшие значения метрики качества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**метод upsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_upsampled_new, target_upsampled_new = upsample(features_train_new, target_train_new, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    4404\n",
      "0    4353\n",
      "Name: exited, dtype: int64\n",
      "0    4353\n",
      "1    1101\n",
      "Name: exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(target_upsampled_new.value_counts())\n",
    "print(target_train_new.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели на валидационной выборке: 0.5922651933701657 Количество деревьев: 38 Максимальная глубина: 5 Максимальный AUC_ROC: 0.8401608336080064\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(1, 100):\n",
    "    for depth in range(1, 6):\n",
    "        model = RandomForestClassifier(random_state=123, n_estimators=est, max_depth = depth)\n",
    "        model.fit(features_upsampled_new, target_upsampled_new)\n",
    "        predicted_valid_new = model.predict(features_valid_new)\n",
    "        result = f1_score(target_valid_new, predicted_valid_new)\n",
    "        probabilities_valid_new = model.predict_proba(features_valid_new)\n",
    "        probabilities_one_valid_new = probabilities_valid_new[:, 1]\n",
    "        auc_roc = roc_auc_score(target_valid_new, probabilities_one_valid_new)\n",
    "        if result > best_result:\n",
    "            best_model = model \n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            best_auc_roc = auc_roc\n",
    "            \n",
    "print(\"F1-мера наилучшей модели на валидационной выборке:\", best_result, \"Количество деревьев:\", best_est, \"Максимальная глубина:\", best_depth, \"Максимальный AUC_ROC:\", best_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимое качство метрики для данного метода удовлетворяет условию задачи, но на всякий случай проверим также взвешивание классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Взвешивание классов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели на валидационной выборке: 0.5856980703745743 Количество деревьев: 6 Максимальная глубина: 4 Максимальный AUC_ROC: 0.8337737467413777\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(1, 11):\n",
    "    for depth in range(1, 6):\n",
    "        model = RandomForestClassifier(random_state=123, n_estimators=est, max_depth = depth, class_weight = \"balanced\")\n",
    "        model.fit(features_train_new, target_train_new)\n",
    "        predicted_valid_new = model.predict(features_valid_new)\n",
    "        result = f1_score(target_valid_new, predicted_valid_new)\n",
    "        probabilities_valid_new = model.predict_proba(features_valid_new)\n",
    "        probabilities_one_valid_new = probabilities_valid_new[:, 1]\n",
    "        auc_roc = roc_auc_score(target_valid_new, probabilities_one_valid_new)\n",
    "        if result > best_result:\n",
    "            best_model = model \n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            best_auc_roc = auc_roc\n",
    "            \n",
    "print(\"F1-мера наилучшей модели на валидационной выборке:\", best_result, \"Количество деревьев:\", best_est, \"Максимальная глубина:\", best_depth, \"Максимальный AUC_ROC:\", best_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная метрика близка к уловию задачи, но не подходит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Таким образом, с несбалансированными данными была произведена работа тремя методами:upsampling, downsampling и взвешивание классов для обеих выборок. Лучше всего себя показали методы:**\n",
    "- upsampling для модели случайный лес для датасета, в котором были удалены пропущенные значения - метрика = 0.592;\n",
    "- upsampling для модели случайный лес для датасета, в котором были заменены пропущенные значения - метрика = 0.582;\n",
    "- downsampling для модели случайный лес для датасета, в котором были заменены пропущенные значения - метрика = 0.583;\n",
    "- downsampling для модели случайный лес для датасета, в котором были удалены пропущенные значения - метрика = 0.586;\n",
    "- взвешивание для модели случайный лес для датасета, в котором были удалены пропущенные значения - метрика = 0.5857.\n",
    "\n",
    "**Удовлетворяет необходимому условию задачи только первый вариант, однако для проверки, протестируем на тестовой выборке все 5 вариантов.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для начала проверим второй датасет с удаленными пропущенными значениями**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель со значением метрики на валидационной выборке = 0.592 - методо upsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5890985324947589\n",
      "AUC_ROC: 0.8481371385069919\n"
     ]
    }
   ],
   "source": [
    "model_best = RandomForestClassifier(random_state=123, n_estimators=38, max_depth = 5)\n",
    "model_best.fit(features_upsampled_new, target_upsampled_new)\n",
    "predicted_test_new = model_best.predict(features_test_new)\n",
    "result = f1_score(target_test_new, predicted_test_new)\n",
    "print(\"F1:\", result)\n",
    "probabilities_test_new = model_best.predict_proba(features_test_new)\n",
    "probabilities_one_test_new = probabilities_test_new[:, 1]\n",
    "auc_roc = roc_auc_score(target_test_new, probabilities_one_test_new)\n",
    "print(\"AUC_ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить тестовая выборка не прошла пороговое значение 0.59, проверим метрику для метода downsampling для того же датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель со значением метрики на валидационной выборке = 0.586 - метод dowmsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5813221406086044\n",
      "AUC_ROC: 0.8459974080350912\n"
     ]
    }
   ],
   "source": [
    "model_best = RandomForestClassifier(random_state=123, n_estimators=14, max_depth = 5)\n",
    "model_best.fit(features_downsampled_new, target_downsampled_new)\n",
    "predicted_test_new = model_best.predict(features_test_new)\n",
    "result = f1_score(target_test_new, predicted_test_new)\n",
    "print(\"F1:\", result)\n",
    "probabilities_test_new = model_best.predict_proba(features_test_new)\n",
    "probabilities_one_test_new = probabilities_test_new[:, 1]\n",
    "auc_roc = roc_auc_score(target_test_new, probabilities_one_test_new)\n",
    "print(\"AUC_ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка на тестовой выборке также не прошла пороговое значение, проверим метод взвешивание классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель со значением метрики на валидационной выборке = 0.586 - метод взвешивание классов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.576307363927428\n",
      "AUC_ROC: 0.8353884775378146\n"
     ]
    }
   ],
   "source": [
    "model_best = RandomForestClassifier(random_state=123, n_estimators=6, max_depth = 4, class_weight = 'balanced')\n",
    "model_best.fit(features_train_new, target_train_new)\n",
    "predicted_test_new = model.predict(features_test_new)\n",
    "result = f1_score(target_test_new, predicted_test_new)\n",
    "print(\"F1:\", result)\n",
    "probabilities_test_new = model.predict_proba(features_test_new)\n",
    "probabilities_one_test_new = probabilities_test_new[:, 1]\n",
    "auc_roc = roc_auc_score(target_test_new, probabilities_one_test_new)\n",
    "print(\"AUC_ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка на тестовой выборке также не прошла пороговое значение, проверим другой датасет, с заменой пропущенных значений и метод upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель со значением метрики на валидационной выборке = 0.582 - метод upsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.6109452736318409\n",
      "AUC_ROC: 0.8567281443804333\n"
     ]
    }
   ],
   "source": [
    "model_best = RandomForestClassifier(random_state=123, n_estimators=28, max_depth = 5)\n",
    "model_best.fit(features_upsampled_first, target_upsampled_first)\n",
    "predicted_test_first = model_best.predict(features_test_first)\n",
    "result = f1_score(target_test_first, predicted_test_first)\n",
    "print(\"F1:\", result)\n",
    "probabilities_test_first = model_best.predict_proba(features_test_first)\n",
    "probabilities_one_test_first = probabilities_test_first[:, 1]\n",
    "auc_roc = roc_auc_score(target_test_first, probabilities_one_test_first)\n",
    "print(\"AUC_ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка на тестовой выборке прошла пороговое значение в 0.59, но на всякий случай проверим метод downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель со значением метрики на валидационной выборке = 0.583 - метод downsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5741444866920152\n",
      "AUC_ROC: 0.8344134282557628\n"
     ]
    }
   ],
   "source": [
    "model_best = RandomForestClassifier(random_state=123, n_estimators=5, max_depth = 5)\n",
    "model_best.fit(features_downsampled_first, target_downsampled_first)\n",
    "predicted_test_first = model_best.predict(features_test_first)\n",
    "result = f1_score(target_test_first, predicted_test_first)\n",
    "print(\"F1:\", result)\n",
    "probabilities_test_first = model_best.predict_proba(features_test_first)\n",
    "probabilities_one_test_first = probabilities_test_first[:, 1]\n",
    "auc_roc = roc_auc_score(target_test_first, probabilities_one_test_first)\n",
    "print(\"AUC_ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка на тестовой выборке не прошла пороговое значение в 0.59."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Таким образом, несмотря на то, что при проверке на валидационной выборке лучше всего себя показала модель случайный лес на датасете с удаленными пропущенными значениями и метод upsampling, на тестовых выборках лучше всего себя показал датасет с заменой пропущенных значений с методом upsampling и моделью случайный лес. Его показатели качества: F1-мера = 0.61, AUC_ROC = 0.857, что удовлетворяет условиям задачи.**\n",
    "\n",
    "**Для предсказания ухода клиента из банка можно использовать модель случайный лес, в которой данные сбалансированы методом upsampling и пропущенные значения заменены на медиану**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет с удалением признака \"Tenure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      credit_score geography  gender  age    balance  num_of_products  \\\n",
       "0              619    France  Female   42       0.00                1   \n",
       "1              608     Spain  Female   41   83807.86                1   \n",
       "2              502    France  Female   42  159660.80                3   \n",
       "3              699    France  Female   39       0.00                2   \n",
       "4              850     Spain  Female   43  125510.82                1   \n",
       "...            ...       ...     ...  ...        ...              ...   \n",
       "9995           771    France    Male   39       0.00                2   \n",
       "9996           516    France    Male   35   57369.61                1   \n",
       "9997           709    France  Female   36       0.00                1   \n",
       "9998           772   Germany    Male   42   75075.31                2   \n",
       "9999           792    France  Female   28  130142.79                1   \n",
       "\n",
       "      has_cr_card  is_active_member  estimated_salary  exited  \n",
       "0               1                 1         101348.88       1  \n",
       "1               0                 1         112542.58       0  \n",
       "2               1                 0         113931.57       1  \n",
       "3               0                 0          93826.63       0  \n",
       "4               1                 1          79084.10       0  \n",
       "...           ...               ...               ...     ...  \n",
       "9995            1                 0          96270.64       0  \n",
       "9996            1                 1         101699.77       0  \n",
       "9997            0                 1          42085.58       1  \n",
       "9998            1                 0          92888.52       1  \n",
       "9999            1                 0          38190.78       0  \n",
       "\n",
       "[10000 rows x 10 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop = df.drop(['tenure'], axis = 1)\n",
    "print(df_drop.shape)\n",
    "df_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_ohe = pd.get_dummies(df_drop, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 11)\n",
      "(2000, 11)\n"
     ]
    }
   ],
   "source": [
    "train_drop = df_drop_ohe.sample(frac = 0.8, random_state = 123).copy()\n",
    "print(train_drop.shape)\n",
    "\n",
    "test_drop = df_drop_ohe[~df_drop_ohe.index.isin(train_drop.index)].copy()\n",
    "print(test_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 11)\n",
      "(2000, 11)\n"
     ]
    }
   ],
   "source": [
    "df_train_drop, df_valid_drop = train_test_split(train_drop, test_size=0.25, random_state=123)\n",
    "print(df_train_drop.shape)\n",
    "print(df_valid_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 10)\n",
      "(6000,)\n",
      "(2000, 10)\n",
      "(2000,)\n",
      "(2000, 10)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "features_train_drop = df_train_drop.drop(['exited'], axis=1)\n",
    "target_train_drop = df_train_drop['exited']\n",
    "\n",
    "features_valid_drop = df_valid_drop.drop(['exited'], axis=1)\n",
    "target_valid_drop = df_valid_drop['exited']\n",
    "\n",
    "features_test_drop = test_drop.drop(['exited'], axis=1)\n",
    "target_test_drop = test_drop['exited']\n",
    "\n",
    "print(features_train_drop.shape)\n",
    "print(target_train_drop.shape)\n",
    "\n",
    "print(features_valid_drop.shape)\n",
    "print(target_valid_drop.shape)\n",
    "\n",
    "print(features_test_drop.shape)\n",
    "print(target_test_drop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>geography_Germany</th>\n",
       "      <th>geography_Spain</th>\n",
       "      <th>gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2256</td>\n",
       "      <td>0.479630</td>\n",
       "      <td>-0.568592</td>\n",
       "      <td>1.393707</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.609849</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9202</td>\n",
       "      <td>-1.257624</td>\n",
       "      <td>-0.759496</td>\n",
       "      <td>0.715344</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.638045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8544</td>\n",
       "      <td>0.407245</td>\n",
       "      <td>0.672285</td>\n",
       "      <td>-0.393582</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.459033</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8818</td>\n",
       "      <td>0.283155</td>\n",
       "      <td>-0.186784</td>\n",
       "      <td>-1.230201</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.065188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8017</td>\n",
       "      <td>-1.971139</td>\n",
       "      <td>1.054093</td>\n",
       "      <td>0.529178</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984366</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      credit_score       age   balance  num_of_products  has_cr_card  \\\n",
       "2256      0.479630 -0.568592  1.393707                1            1   \n",
       "9202     -1.257624 -0.759496  0.715344                1            1   \n",
       "8544      0.407245  0.672285 -0.393582                2            1   \n",
       "8818      0.283155 -0.186784 -1.230201                2            1   \n",
       "8017     -1.971139  1.054093  0.529178                1            1   \n",
       "\n",
       "      is_active_member  estimated_salary  geography_Germany  geography_Spain  \\\n",
       "2256                 1         -1.609849                  0                0   \n",
       "9202                 0         -1.638045                  0                0   \n",
       "8544                 1         -0.459033                  0                0   \n",
       "8818                 1         -1.065188                  0                0   \n",
       "8017                 0          0.984366                  1                0   \n",
       "\n",
       "      gender_Male  \n",
       "2256            1  \n",
       "9202            1  \n",
       "8544            0  \n",
       "8818            0  \n",
       "8017            1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Выделяем те признаки, которые необходимо масштабировать\n",
    "numeric = ['credit_score', 'age', 'balance', 'estimated_salary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train_drop[numeric])\n",
    "features_train_drop[numeric] =  scaler.transform(features_train_drop[numeric])\n",
    "features_valid_drop[numeric] =  scaler.transform(features_valid_drop[numeric])\n",
    "features_test_drop[numeric] =  scaler.transform(features_test_drop[numeric])\n",
    "features_train_drop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Взвешивание классов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели на валидационной выборке: 0.5773195876288659 Количество деревьев: 10 Максимальная глубина: 5 Максимальный AUC_ROC: 0.8440846424120934\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(1, 11):\n",
    "    for depth in range(1, 6):\n",
    "        model = RandomForestClassifier(random_state=123, n_estimators=est, max_depth = depth, class_weight = \"balanced\")\n",
    "        model.fit(features_train_drop, target_train_drop)\n",
    "        predicted_valid_drop = model.predict(features_valid_drop)\n",
    "        result = f1_score(target_valid_drop, predicted_valid_drop)\n",
    "        probabilities_valid_drop = model.predict_proba(features_valid_drop)\n",
    "        probabilities_one_valid_drop = probabilities_valid_drop[:, 1]\n",
    "        auc_roc = roc_auc_score(target_valid_drop, probabilities_one_valid_drop)\n",
    "        if result > best_result:\n",
    "            best_model = model \n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            best_auc_roc = auc_roc\n",
    "            \n",
    "print(\"F1-мера наилучшей модели на валидационной выборке:\", best_result, \"Количество деревьев:\", best_est, \"Максимальная глубина:\", best_depth, \"Максимальный AUC_ROC:\", best_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод upsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    4884\n",
      "0    4779\n",
      "Name: exited, dtype: int64\n",
      "0    4779\n",
      "1    1221\n",
      "Name: exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "features_upsampled_drop, target_upsampled_drop = upsample(features_train_drop, target_train_drop, 4)\n",
    "print(target_upsampled_drop.value_counts())\n",
    "print(target_train_drop.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели на валидационной выборке: 0.5798575788402849 Количество деревьев: 99 Максимальная глубина: 5 Максимальный AUC_ROC: 0.8489401489349238\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(1, 100):\n",
    "    for depth in range(1, 6):\n",
    "        model = RandomForestClassifier(random_state=123, n_estimators=est, max_depth = depth)\n",
    "        model.fit(features_upsampled_drop, target_upsampled_drop)\n",
    "        predicted_valid_drop = model.predict(features_valid_drop)\n",
    "        result = f1_score(target_valid_drop, predicted_valid_drop)\n",
    "        probabilities_valid_drop = model.predict_proba(features_valid_drop)\n",
    "        probabilities_one_valid_drop = probabilities_valid_drop[:, 1]\n",
    "        auc_roc = roc_auc_score(target_valid_drop, probabilities_one_valid_drop)\n",
    "        if result > best_result:\n",
    "            best_model = model \n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            best_auc_roc = auc_roc\n",
    "            \n",
    "print(\"F1-мера наилучшей модели на валидационной выборке:\", best_result, \"Количество деревьев:\", best_est, \"Максимальная глубина:\", best_depth, \"Максимальный AUC_ROC:\", best_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод downsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1221\n",
      "0    1195\n",
      "Name: exited, dtype: int64\n",
      "0    4779\n",
      "1    1221\n",
      "Name: exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "features_downsampled_drop, target_downsampled_drop = downsample(features_train_drop, target_train_drop, 0.25)\n",
    "print(target_downsampled_drop.value_counts())\n",
    "print(target_train_drop.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера наилучшей модели на валидационной выборке: 0.5783836416747808 Количество деревьев: 24 Максимальная глубина: 5 Максимальный AUC_ROC: 0.8452294430695224\n"
     ]
    }
   ],
   "source": [
    "# Проверяем качество модели на валидационной выборке\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(1, 100):\n",
    "    for depth in range(1, 6):\n",
    "        model = RandomForestClassifier(random_state=123, n_estimators=est, max_depth = depth)\n",
    "        model.fit(features_downsampled_drop, target_downsampled_drop)\n",
    "        predicted_valid_drop = model.predict(features_valid_drop)\n",
    "        result = f1_score(target_valid_drop, predicted_valid_drop)\n",
    "        probabilities_valid_drop = model.predict_proba(features_valid_drop)\n",
    "        probabilities_one_valid_drop = probabilities_valid_drop[:, 1]\n",
    "        auc_roc = roc_auc_score(target_valid_drop, probabilities_one_valid_drop)\n",
    "        if result > best_result:\n",
    "            best_model = model \n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            best_auc_roc = auc_roc\n",
    "            \n",
    "print(\"F1-мера наилучшей модели на валидационной выборке:\", best_result, \"Количество деревьев:\", best_est, \"Максимальная глубина:\", best_depth, \"Максимальный AUC_ROC:\", best_auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тестирование метод upsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.6068292682926829\n",
      "AUC_ROC: 0.8618460403765116\n"
     ]
    }
   ],
   "source": [
    "model_best = RandomForestClassifier(random_state=123, n_estimators=99, max_depth = 5)\n",
    "model_best.fit(features_upsampled_drop, target_upsampled_drop)\n",
    "predicted_test_drop = model_best.predict(features_test_drop)\n",
    "result = f1_score(target_test_drop, predicted_test_drop)\n",
    "print(\"F1:\", result)\n",
    "probabilities_test_drop = model_best.predict_proba(features_test_drop)\n",
    "probabilities_one_test_drop = probabilities_test_drop[:, 1]\n",
    "auc_roc = roc_auc_score(target_test_drop, probabilities_one_test_drop)\n",
    "print(\"AUC_ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тестирование метод downsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5964912280701754\n",
      "AUC_ROC: 0.856040811247978\n"
     ]
    }
   ],
   "source": [
    "model_best = RandomForestClassifier(random_state=123, n_estimators=24, max_depth = 5)\n",
    "model_best.fit(features_downsampled_drop, target_downsampled_drop)\n",
    "predicted_test_drop = model_best.predict(features_test_drop)\n",
    "result = f1_score(target_test_drop, predicted_test_drop)\n",
    "print(\"F1:\", result)\n",
    "probabilities_test_drop = model_best.predict_proba(features_test_drop)\n",
    "probabilities_one_test_drop = probabilities_test_drop[:, 1]\n",
    "auc_roc = roc_auc_score(target_test_drop, probabilities_one_test_drop)\n",
    "print(\"AUC_ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тестирование метод взвешивание**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5946969696969697\n",
      "AUC_ROC: 0.8597630537079262\n"
     ]
    }
   ],
   "source": [
    "model_best = RandomForestClassifier(random_state=123, n_estimators=6, max_depth = 4, class_weight = 'balanced')\n",
    "model_best.fit(features_train_drop, target_train_drop)\n",
    "predicted_test_drop = model.predict(features_test_drop)\n",
    "result = f1_score(target_test_drop, predicted_test_drop)\n",
    "print(\"F1:\", result)\n",
    "probabilities_test_drop = model.predict_proba(features_test_drop)\n",
    "probabilities_one_test_drop = probabilities_test_drop[:, 1]\n",
    "auc_roc = roc_auc_score(target_test_drop, probabilities_one_test_drop)\n",
    "print(\"AUC_ROC:\", auc_roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Таким образом, можно сказать, что удаление признака \"tenure\" полностью, значительно повлияло на показатели качества модели для тестовой выборки, при всех методах достигается положительный результат качества, который удовлетворяет условию задачи, но лучшим показателем стала модель случайный лес, при методе борьбы с дисбалансом upsampling, с мерой F1 = 0.607**"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 336,
    "start_time": "2021-06-08T18:57:25.277Z"
   },
   {
    "duration": 1388,
    "start_time": "2021-06-08T18:57:36.387Z"
   },
   {
    "duration": 103,
    "start_time": "2021-06-08T18:57:37.778Z"
   },
   {
    "duration": 49,
    "start_time": "2021-06-08T18:57:37.884Z"
   },
   {
    "duration": 5,
    "start_time": "2021-06-08T18:57:37.936Z"
   },
   {
    "duration": 13,
    "start_time": "2021-06-08T18:57:37.943Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-08T18:57:37.959Z"
   },
   {
    "duration": 40,
    "start_time": "2021-06-08T18:57:38.000Z"
   },
   {
    "duration": 12,
    "start_time": "2021-06-08T18:57:38.043Z"
   },
   {
    "duration": 7,
    "start_time": "2021-06-08T18:57:38.059Z"
   },
   {
    "duration": 15,
    "start_time": "2021-06-08T18:57:38.099Z"
   },
   {
    "duration": 43,
    "start_time": "2021-06-08T18:57:38.118Z"
   },
   {
    "duration": 37,
    "start_time": "2021-06-08T18:57:38.163Z"
   },
   {
    "duration": 34,
    "start_time": "2021-06-08T18:57:38.203Z"
   },
   {
    "duration": 154,
    "start_time": "2021-06-08T18:57:38.240Z"
   },
   {
    "duration": 106,
    "start_time": "2021-06-08T18:57:38.400Z"
   },
   {
    "duration": 193,
    "start_time": "2021-06-08T18:57:38.508Z"
   },
   {
    "duration": 12,
    "start_time": "2021-06-08T18:57:38.704Z"
   },
   {
    "duration": 13,
    "start_time": "2021-06-08T18:57:38.719Z"
   },
   {
    "duration": 74,
    "start_time": "2021-06-08T18:57:38.735Z"
   },
   {
    "duration": 31,
    "start_time": "2021-06-08T18:57:38.812Z"
   },
   {
    "duration": 139,
    "start_time": "2021-06-08T18:57:38.845Z"
   },
   {
    "duration": 109,
    "start_time": "2021-06-08T18:57:38.999Z"
   },
   {
    "duration": 247,
    "start_time": "2021-06-08T18:57:39.116Z"
   },
   {
    "duration": 2105,
    "start_time": "2021-06-08T19:04:49.686Z"
   },
   {
    "duration": 1833,
    "start_time": "2021-06-08T19:10:51.977Z"
   },
   {
    "duration": 93,
    "start_time": "2021-06-08T19:15:34.119Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-08T19:28:22.848Z"
   },
   {
    "duration": 7,
    "start_time": "2021-06-08T19:28:34.151Z"
   },
   {
    "duration": 619,
    "start_time": "2021-06-08T19:31:46.796Z"
   },
   {
    "duration": 1360,
    "start_time": "2021-06-08T19:32:00.843Z"
   },
   {
    "duration": 106,
    "start_time": "2021-06-08T19:32:02.206Z"
   },
   {
    "duration": 48,
    "start_time": "2021-06-08T19:32:02.315Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-08T19:32:02.366Z"
   },
   {
    "duration": 37,
    "start_time": "2021-06-08T19:32:02.374Z"
   },
   {
    "duration": 13,
    "start_time": "2021-06-08T19:32:02.414Z"
   },
   {
    "duration": 70,
    "start_time": "2021-06-08T19:32:02.431Z"
   },
   {
    "duration": 16,
    "start_time": "2021-06-08T19:32:02.505Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-08T19:32:02.526Z"
   },
   {
    "duration": 14,
    "start_time": "2021-06-08T19:32:02.539Z"
   },
   {
    "duration": 98,
    "start_time": "2021-06-08T19:32:02.555Z"
   },
   {
    "duration": 8,
    "start_time": "2021-06-08T19:32:02.656Z"
   },
   {
    "duration": 49,
    "start_time": "2021-06-08T19:32:02.667Z"
   },
   {
    "duration": 147,
    "start_time": "2021-06-08T19:32:02.720Z"
   },
   {
    "duration": 57,
    "start_time": "2021-06-08T19:32:02.870Z"
   },
   {
    "duration": 103,
    "start_time": "2021-06-08T19:32:03.004Z"
   },
   {
    "duration": 12,
    "start_time": "2021-06-08T19:32:03.110Z"
   },
   {
    "duration": 16,
    "start_time": "2021-06-08T19:32:03.126Z"
   },
   {
    "duration": 89,
    "start_time": "2021-06-08T19:32:03.145Z"
   },
   {
    "duration": 27,
    "start_time": "2021-06-08T19:32:03.237Z"
   },
   {
    "duration": 138,
    "start_time": "2021-06-08T19:32:03.266Z"
   },
   {
    "duration": 100,
    "start_time": "2021-06-08T19:32:03.407Z"
   },
   {
    "duration": 276,
    "start_time": "2021-06-08T19:32:03.510Z"
   },
   {
    "duration": 2011,
    "start_time": "2021-06-08T19:32:03.801Z"
   },
   {
    "duration": 93,
    "start_time": "2021-06-08T19:32:05.815Z"
   },
   {
    "duration": 195,
    "start_time": "2021-06-08T19:32:05.915Z"
   },
   {
    "duration": 439,
    "start_time": "2021-06-08T19:33:54.405Z"
   },
   {
    "duration": 288,
    "start_time": "2021-06-08T19:34:16.571Z"
   },
   {
    "duration": 9184,
    "start_time": "2021-06-08T19:35:14.773Z"
   },
   {
    "duration": 24,
    "start_time": "2021-06-08T19:37:04.695Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-08T19:37:40.898Z"
   },
   {
    "duration": 8,
    "start_time": "2021-06-08T19:38:02.799Z"
   },
   {
    "duration": 8,
    "start_time": "2021-06-08T19:38:11.672Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-08T19:38:23.617Z"
   },
   {
    "duration": 127,
    "start_time": "2021-06-08T19:42:37.343Z"
   },
   {
    "duration": 134,
    "start_time": "2021-06-08T19:47:04.384Z"
   },
   {
    "duration": 1336,
    "start_time": "2021-06-08T19:47:28.705Z"
   },
   {
    "duration": 102,
    "start_time": "2021-06-08T19:47:30.044Z"
   },
   {
    "duration": 54,
    "start_time": "2021-06-08T19:47:30.150Z"
   },
   {
    "duration": 8,
    "start_time": "2021-06-08T19:47:30.208Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-08T19:47:30.218Z"
   },
   {
    "duration": 12,
    "start_time": "2021-06-08T19:47:30.231Z"
   },
   {
    "duration": 75,
    "start_time": "2021-06-08T19:47:30.246Z"
   },
   {
    "duration": 13,
    "start_time": "2021-06-08T19:47:30.324Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-08T19:47:30.341Z"
   },
   {
    "duration": 14,
    "start_time": "2021-06-08T19:47:30.353Z"
   },
   {
    "duration": 46,
    "start_time": "2021-06-08T19:47:30.401Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-08T19:47:30.449Z"
   },
   {
    "duration": 69,
    "start_time": "2021-06-08T19:47:30.460Z"
   },
   {
    "duration": 135,
    "start_time": "2021-06-08T19:47:30.531Z"
   },
   {
    "duration": 140,
    "start_time": "2021-06-08T19:47:30.668Z"
   },
   {
    "duration": 107,
    "start_time": "2021-06-08T19:47:30.811Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-08T19:47:30.920Z"
   },
   {
    "duration": 14,
    "start_time": "2021-06-08T19:47:30.932Z"
   },
   {
    "duration": 75,
    "start_time": "2021-06-08T19:47:30.948Z"
   },
   {
    "duration": 27,
    "start_time": "2021-06-08T19:47:31.026Z"
   },
   {
    "duration": 140,
    "start_time": "2021-06-08T19:47:31.056Z"
   },
   {
    "duration": 110,
    "start_time": "2021-06-08T19:47:31.202Z"
   },
   {
    "duration": 292,
    "start_time": "2021-06-08T19:47:31.317Z"
   },
   {
    "duration": 1819,
    "start_time": "2021-06-08T19:47:31.612Z"
   },
   {
    "duration": 78,
    "start_time": "2021-06-08T19:47:33.434Z"
   },
   {
    "duration": 114,
    "start_time": "2021-06-08T19:47:33.600Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-08T19:47:33.720Z"
   },
   {
    "duration": 154,
    "start_time": "2021-06-08T19:47:33.733Z"
   },
   {
    "duration": 2388,
    "start_time": "2021-06-08T19:51:05.553Z"
   },
   {
    "duration": 102,
    "start_time": "2021-06-08T19:53:05.175Z"
   },
   {
    "duration": 2432,
    "start_time": "2021-06-08T19:53:09.426Z"
   },
   {
    "duration": 2440,
    "start_time": "2021-06-08T19:54:15.684Z"
   },
   {
    "duration": 128,
    "start_time": "2021-06-08T19:55:11.633Z"
   },
   {
    "duration": 115,
    "start_time": "2021-06-08T19:58:35.092Z"
   },
   {
    "duration": 17,
    "start_time": "2021-06-08T20:07:36.833Z"
   },
   {
    "duration": 454,
    "start_time": "2021-06-08T20:07:50.362Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-08T20:08:25.273Z"
   },
   {
    "duration": 72,
    "start_time": "2021-06-08T20:10:02.133Z"
   },
   {
    "duration": 1267,
    "start_time": "2021-06-08T20:13:28.967Z"
   },
   {
    "duration": 7341,
    "start_time": "2021-06-08T20:14:47.636Z"
   },
   {
    "duration": 78753,
    "start_time": "2021-06-08T20:15:08.934Z"
   },
   {
    "duration": 152430,
    "start_time": "2021-06-08T20:18:10.213Z"
   },
   {
    "duration": 67,
    "start_time": "2021-06-08T20:21:18.633Z"
   },
   {
    "duration": 95,
    "start_time": "2021-06-08T20:21:57.713Z"
   },
   {
    "duration": 17,
    "start_time": "2021-06-08T20:26:23.793Z"
   },
   {
    "duration": 11,
    "start_time": "2021-06-08T20:26:52.732Z"
   },
   {
    "duration": 76885,
    "start_time": "2021-06-08T20:28:18.593Z"
   },
   {
    "duration": 340,
    "start_time": "2021-06-08T20:33:44.633Z"
   },
   {
    "duration": 795,
    "start_time": "2021-06-08T20:36:46.013Z"
   },
   {
    "duration": 324,
    "start_time": "2021-06-09T09:19:40.642Z"
   },
   {
    "duration": 1374,
    "start_time": "2021-06-09T09:19:47.384Z"
   },
   {
    "duration": 97,
    "start_time": "2021-06-09T09:19:48.761Z"
   },
   {
    "duration": 53,
    "start_time": "2021-06-09T09:19:48.862Z"
   },
   {
    "duration": 5,
    "start_time": "2021-06-09T09:19:48.918Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-09T09:19:48.926Z"
   },
   {
    "duration": 11,
    "start_time": "2021-06-09T09:19:48.938Z"
   },
   {
    "duration": 73,
    "start_time": "2021-06-09T09:19:48.951Z"
   },
   {
    "duration": 14,
    "start_time": "2021-06-09T09:19:49.028Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-09T09:19:49.045Z"
   },
   {
    "duration": 45,
    "start_time": "2021-06-09T09:19:49.057Z"
   },
   {
    "duration": 39,
    "start_time": "2021-06-09T09:19:49.104Z"
   },
   {
    "duration": 7,
    "start_time": "2021-06-09T09:19:49.146Z"
   },
   {
    "duration": 60,
    "start_time": "2021-06-09T09:19:49.155Z"
   },
   {
    "duration": 135,
    "start_time": "2021-06-09T09:19:49.217Z"
   },
   {
    "duration": 149,
    "start_time": "2021-06-09T09:19:49.354Z"
   },
   {
    "duration": 103,
    "start_time": "2021-06-09T09:19:49.507Z"
   },
   {
    "duration": 8,
    "start_time": "2021-06-09T09:19:49.612Z"
   },
   {
    "duration": 13,
    "start_time": "2021-06-09T09:19:49.622Z"
   },
   {
    "duration": 79,
    "start_time": "2021-06-09T09:19:49.637Z"
   },
   {
    "duration": 26,
    "start_time": "2021-06-09T09:19:49.718Z"
   },
   {
    "duration": 131,
    "start_time": "2021-06-09T09:19:49.746Z"
   },
   {
    "duration": 51,
    "start_time": "2021-06-09T09:19:49.880Z"
   },
   {
    "duration": 213,
    "start_time": "2021-06-09T09:19:50.006Z"
   },
   {
    "duration": 1930,
    "start_time": "2021-06-09T09:19:50.222Z"
   },
   {
    "duration": 52,
    "start_time": "2021-06-09T09:19:52.155Z"
   },
   {
    "duration": 194,
    "start_time": "2021-06-09T09:19:52.212Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-09T09:19:52.409Z"
   },
   {
    "duration": 161,
    "start_time": "2021-06-09T09:19:52.422Z"
   },
   {
    "duration": 152214,
    "start_time": "2021-06-09T09:19:52.586Z"
   },
   {
    "duration": 106,
    "start_time": "2021-06-09T09:22:24.802Z"
   },
   {
    "duration": 196,
    "start_time": "2021-06-09T09:22:24.910Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-09T09:22:25.108Z"
   },
   {
    "duration": 98,
    "start_time": "2021-06-09T09:22:25.119Z"
   },
   {
    "duration": 80279,
    "start_time": "2021-06-09T09:22:25.219Z"
   },
   {
    "duration": 105,
    "start_time": "2021-06-09T09:23:45.502Z"
   },
   {
    "duration": 100,
    "start_time": "2021-06-09T09:23:45.610Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-09T09:23:45.712Z"
   },
   {
    "duration": 76314,
    "start_time": "2021-06-09T09:23:45.725Z"
   },
   {
    "duration": 14,
    "start_time": "2021-06-09T09:25:02.041Z"
   },
   {
    "duration": 815,
    "start_time": "2021-06-09T09:25:02.057Z"
   },
   {
    "duration": 23,
    "start_time": "2021-06-09T09:25:51.969Z"
   },
   {
    "duration": 143542,
    "start_time": "2021-06-09T09:28:13.830Z"
   },
   {
    "duration": 1700,
    "start_time": "2021-06-09T09:34:16.558Z"
   },
   {
    "duration": 96,
    "start_time": "2021-06-09T09:50:29.585Z"
   },
   {
    "duration": 55,
    "start_time": "2021-06-09T09:54:11.764Z"
   },
   {
    "duration": 287,
    "start_time": "2021-06-09T09:56:25.284Z"
   },
   {
    "duration": 66,
    "start_time": "2021-06-09T09:57:35.375Z"
   },
   {
    "duration": 59,
    "start_time": "2021-06-09T10:02:44.679Z"
   },
   {
    "duration": 258,
    "start_time": "2021-06-09T10:10:16.029Z"
   },
   {
    "duration": 37,
    "start_time": "2021-06-09T10:13:03.650Z"
   },
   {
    "duration": 1404,
    "start_time": "2021-06-09T10:34:16.631Z"
   },
   {
    "duration": 112,
    "start_time": "2021-06-09T10:34:18.039Z"
   },
   {
    "duration": 57,
    "start_time": "2021-06-09T10:34:18.154Z"
   },
   {
    "duration": 5,
    "start_time": "2021-06-09T10:34:18.215Z"
   },
   {
    "duration": 15,
    "start_time": "2021-06-09T10:34:18.224Z"
   },
   {
    "duration": 15,
    "start_time": "2021-06-09T10:34:18.242Z"
   },
   {
    "duration": 69,
    "start_time": "2021-06-09T10:34:18.260Z"
   },
   {
    "duration": 14,
    "start_time": "2021-06-09T10:34:18.333Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-09T10:34:18.350Z"
   },
   {
    "duration": 56,
    "start_time": "2021-06-09T10:34:18.362Z"
   },
   {
    "duration": 40,
    "start_time": "2021-06-09T10:34:18.421Z"
   },
   {
    "duration": 40,
    "start_time": "2021-06-09T10:34:18.464Z"
   },
   {
    "duration": 31,
    "start_time": "2021-06-09T10:34:18.512Z"
   },
   {
    "duration": 165,
    "start_time": "2021-06-09T10:34:18.546Z"
   },
   {
    "duration": 101,
    "start_time": "2021-06-09T10:34:18.713Z"
   },
   {
    "duration": 97,
    "start_time": "2021-06-09T10:34:18.817Z"
   },
   {
    "duration": 93,
    "start_time": "2021-06-09T10:34:18.917Z"
   },
   {
    "duration": 15,
    "start_time": "2021-06-09T10:34:19.013Z"
   },
   {
    "duration": 82,
    "start_time": "2021-06-09T10:34:19.031Z"
   },
   {
    "duration": 26,
    "start_time": "2021-06-09T10:34:19.117Z"
   },
   {
    "duration": 143,
    "start_time": "2021-06-09T10:34:19.146Z"
   },
   {
    "duration": 117,
    "start_time": "2021-06-09T10:34:19.292Z"
   },
   {
    "duration": 265,
    "start_time": "2021-06-09T10:34:19.418Z"
   },
   {
    "duration": 2214,
    "start_time": "2021-06-09T10:34:19.686Z"
   },
   {
    "duration": 116,
    "start_time": "2021-06-09T10:34:21.905Z"
   },
   {
    "duration": 109,
    "start_time": "2021-06-09T10:34:22.102Z"
   },
   {
    "duration": 20,
    "start_time": "2021-06-09T10:34:22.213Z"
   },
   {
    "duration": 152,
    "start_time": "2021-06-09T10:34:22.236Z"
   },
   {
    "duration": 163175,
    "start_time": "2021-06-09T10:34:22.391Z"
   },
   {
    "duration": 139,
    "start_time": "2021-06-09T10:37:05.569Z"
   },
   {
    "duration": 97,
    "start_time": "2021-06-09T10:37:05.714Z"
   },
   {
    "duration": 14,
    "start_time": "2021-06-09T10:37:05.814Z"
   },
   {
    "duration": 141,
    "start_time": "2021-06-09T10:37:05.830Z"
   },
   {
    "duration": 85521,
    "start_time": "2021-06-09T10:37:05.973Z"
   },
   {
    "duration": 106,
    "start_time": "2021-06-09T10:38:31.496Z"
   },
   {
    "duration": 107,
    "start_time": "2021-06-09T10:38:31.606Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-09T10:38:31.716Z"
   },
   {
    "duration": 81974,
    "start_time": "2021-06-09T10:38:31.728Z"
   },
   {
    "duration": 22,
    "start_time": "2021-06-09T10:39:53.705Z"
   },
   {
    "duration": 8,
    "start_time": "2021-06-09T10:39:53.729Z"
   },
   {
    "duration": 149312,
    "start_time": "2021-06-09T10:39:53.739Z"
   },
   {
    "duration": 1770,
    "start_time": "2021-06-09T10:42:23.054Z"
   },
   {
    "duration": 318,
    "start_time": "2021-06-09T10:42:24.827Z"
   },
   {
    "duration": 79,
    "start_time": "2021-06-09T10:42:25.147Z"
   },
   {
    "duration": 53,
    "start_time": "2021-06-09T10:42:25.229Z"
   },
   {
    "duration": 266,
    "start_time": "2021-06-09T10:42:25.284Z"
   },
   {
    "duration": 50,
    "start_time": "2021-06-09T10:42:25.552Z"
   },
   {
    "duration": 310,
    "start_time": "2021-06-10T13:56:20.357Z"
   },
   {
    "duration": 1409,
    "start_time": "2021-06-10T13:56:29.024Z"
   },
   {
    "duration": 100,
    "start_time": "2021-06-10T13:56:30.436Z"
   },
   {
    "duration": 48,
    "start_time": "2021-06-10T13:56:30.539Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-10T13:56:30.591Z"
   },
   {
    "duration": 20,
    "start_time": "2021-06-10T13:56:30.603Z"
   },
   {
    "duration": 36,
    "start_time": "2021-06-10T13:56:30.626Z"
   },
   {
    "duration": 44,
    "start_time": "2021-06-10T13:56:30.664Z"
   },
   {
    "duration": 13,
    "start_time": "2021-06-10T13:56:30.712Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-10T13:56:30.755Z"
   },
   {
    "duration": 24,
    "start_time": "2021-06-10T13:56:30.767Z"
   },
   {
    "duration": 96,
    "start_time": "2021-06-10T13:56:30.793Z"
   },
   {
    "duration": 13,
    "start_time": "2021-06-10T13:56:30.892Z"
   },
   {
    "duration": 70,
    "start_time": "2021-06-10T13:56:30.908Z"
   },
   {
    "duration": 189,
    "start_time": "2021-06-10T13:56:30.991Z"
   },
   {
    "duration": 174,
    "start_time": "2021-06-10T13:56:31.184Z"
   },
   {
    "duration": 100,
    "start_time": "2021-06-10T13:56:31.361Z"
   },
   {
    "duration": 16,
    "start_time": "2021-06-10T13:56:31.464Z"
   },
   {
    "duration": 14,
    "start_time": "2021-06-10T13:56:31.482Z"
   },
   {
    "duration": 71,
    "start_time": "2021-06-10T13:56:31.499Z"
   },
   {
    "duration": 30,
    "start_time": "2021-06-10T13:56:31.573Z"
   },
   {
    "duration": 129,
    "start_time": "2021-06-10T13:56:31.606Z"
   },
   {
    "duration": 127,
    "start_time": "2021-06-10T13:56:31.737Z"
   },
   {
    "duration": 209,
    "start_time": "2021-06-10T13:56:31.959Z"
   },
   {
    "duration": 1787,
    "start_time": "2021-06-10T13:56:32.171Z"
   },
   {
    "duration": 107,
    "start_time": "2021-06-10T13:56:33.960Z"
   },
   {
    "duration": 108,
    "start_time": "2021-06-10T13:56:34.156Z"
   },
   {
    "duration": 12,
    "start_time": "2021-06-10T13:56:34.266Z"
   },
   {
    "duration": 165,
    "start_time": "2021-06-10T13:56:34.280Z"
   },
   {
    "duration": 156944,
    "start_time": "2021-06-10T13:56:34.447Z"
   },
   {
    "duration": 75,
    "start_time": "2021-06-10T13:59:11.394Z"
   },
   {
    "duration": 109,
    "start_time": "2021-06-10T13:59:11.557Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-10T13:59:11.669Z"
   },
   {
    "duration": 103,
    "start_time": "2021-06-10T13:59:11.681Z"
   },
   {
    "duration": 80633,
    "start_time": "2021-06-10T13:59:11.787Z"
   },
   {
    "duration": 42,
    "start_time": "2021-06-10T14:00:32.422Z"
   },
   {
    "duration": 189,
    "start_time": "2021-06-10T14:00:32.466Z"
   },
   {
    "duration": 11,
    "start_time": "2021-06-10T14:00:32.657Z"
   },
   {
    "duration": 77153,
    "start_time": "2021-06-10T14:00:32.671Z"
   },
   {
    "duration": 27,
    "start_time": "2021-06-10T14:01:49.827Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-10T14:01:49.856Z"
   },
   {
    "duration": 149793,
    "start_time": "2021-06-10T14:01:49.869Z"
   },
   {
    "duration": 1746,
    "start_time": "2021-06-10T14:04:19.664Z"
   },
   {
    "duration": 307,
    "start_time": "2021-06-10T14:04:21.412Z"
   },
   {
    "duration": 77,
    "start_time": "2021-06-10T14:04:21.721Z"
   },
   {
    "duration": 64,
    "start_time": "2021-06-10T14:04:21.800Z"
   },
   {
    "duration": 246,
    "start_time": "2021-06-10T14:04:21.866Z"
   },
   {
    "duration": 48,
    "start_time": "2021-06-10T14:04:22.115Z"
   },
   {
    "duration": 387,
    "start_time": "2021-06-10T14:04:22.165Z"
   },
   {
    "duration": 7,
    "start_time": "2021-06-10T14:04:50.414Z"
   },
   {
    "duration": 37,
    "start_time": "2021-06-10T14:05:04.423Z"
   },
   {
    "duration": 19,
    "start_time": "2021-06-10T14:06:03.593Z"
   },
   {
    "duration": 15,
    "start_time": "2021-06-10T14:18:21.983Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-10T14:19:28.533Z"
   },
   {
    "duration": 12,
    "start_time": "2021-06-10T14:20:24.336Z"
   },
   {
    "duration": 41,
    "start_time": "2021-06-10T14:21:22.773Z"
   },
   {
    "duration": 30,
    "start_time": "2021-06-10T14:21:32.593Z"
   },
   {
    "duration": 37,
    "start_time": "2021-06-10T14:21:45.393Z"
   },
   {
    "duration": 1766,
    "start_time": "2021-06-10T14:25:57.427Z"
   },
   {
    "duration": 21,
    "start_time": "2021-06-10T14:27:34.606Z"
   },
   {
    "duration": 157992,
    "start_time": "2021-06-10T14:28:44.947Z"
   },
   {
    "duration": 19,
    "start_time": "2021-06-10T14:31:36.490Z"
   },
   {
    "duration": 81369,
    "start_time": "2021-06-10T14:31:44.271Z"
   },
   {
    "duration": 788,
    "start_time": "2021-06-10T14:37:02.243Z"
   },
   {
    "duration": 104,
    "start_time": "2021-06-10T14:37:06.019Z"
   },
   {
    "duration": 107,
    "start_time": "2021-06-10T14:37:09.186Z"
   },
   {
    "duration": 15,
    "start_time": "2021-06-10T14:53:21.588Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
